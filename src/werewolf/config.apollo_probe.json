{
  "backend_type": "probe",
  "detector_path": null,
  "provide_probe_scores": true,
  "use_quantized": false,
  "quantization_bits": 8,
  "output_dir": "/workspace/werewolf_game",
  "max_turns": 10,
  "_comment_detector_path": "If null, uses default: deception-detection/example_results/roleplaying/detector.pt",
  "_comment_provide_probe_scores": "If true, shows probe scores to other players as 'lie detector' readings",
  "_comment_use_quantized": "If true, loads model with quantization to save memory (requires bitsandbytes)",
  "_comment_quantization_bits": "4 or 8 bit quantization. 8-bit recommended (~70GB), 4-bit more aggressive (~35GB)",
  "_comment_memory": "FP16 needs ~140GB, 8-bit needs ~70GB, 4-bit needs ~35GB. All estimates plus ~20-40GB overhead"
}
